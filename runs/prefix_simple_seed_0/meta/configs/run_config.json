{
  "data": {
    "dataset": {
      "hf_id": "HuggingFaceFW/fineweb-edu",
      "name": "sample-100BT",
      "split": "train"
    },
    "dir": "./data/mds/",
    "packing": {
      "sequence_length": 2048
    },
    "text_processing": {
      "batch_size": 2048,
      "num_proc": 64
    },
    "tokenizer": {
      "hf_id": "meta-llama/Meta-Llama-3-8B"
    }
  },
  "eval": {
    "every_steps": 1000,
    "lm_eval": {
      "limit": 100,
      "tasks": [
        "piqa",
        "winogrande",
        "arc_easy",
        "hellaswag",
        "charbench"
      ]
    }
  },
  "model": {
    "architecture": {
      "hidden_size": 1536,
      "intermediate_size": 4096,
      "max_position_embeddings": 2048,
      "num_attention_heads": 12,
      "num_hidden_layers": 12,
      "num_key_value_heads": 4,
      "rms_norm_eps": 1e-05,
      "rope_theta": 500000,
      "tie_word_embeddings": true
    },
    "tokenizer": {
      "type": "hf_llama3"
    }
  },
  "objective": {
    "epsilon": 0.1,
    "type": "prefix_simple"
  },
  "run": {
    "name": "prefix_simple_seed_0",
    "seed": 0
  },
  "train": {
    "adam_betas": [
      0.9,
      0.95
    ],
    "adam_eps": 1e-08,
    "checkpointing": {
      "enabled": true,
      "keep_last": 1,
      "save_every_steps": 1000
    },
    "distributed": {
      "backend": "nccl",
      "world_size": 8
    },
    "grad_clip": 1.0,
    "learning_rate": 0.0002,
    "lr_scheduler": "cosine",
    "max_steps": -1,
    "max_tokens": 10000000000,
    "per_gpu_batch_size": 8,
    "shuffle": true,
    "warmup_steps": 1000,
    "weight_decay": 0.1
  }
}