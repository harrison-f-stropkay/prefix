run:
  name: tiny_ws8
  seed: 0
  type: ce
data:
  dataset:
    hf_id: HuggingFaceFW/fineweb-edu
    name: sample-100BT
    split: train
  tokenizer:
    hf_id: meta-llama/Meta-Llama-3-8B
  text_processing:
    num_proc: 1
    batch_size: 32
  packing:
    sequence_length: 256
  dir: ./data/tiny_mds/
model:
  architecture:
    hidden_size: 256
    num_hidden_layers: 2
    num_attention_heads: 4
    num_key_value_heads: 2
    intermediate_size: 512
    max_position_embeddings: 256
    rope_theta: 500000
    rms_norm_eps: 1.0e-05
    tie_word_embeddings: true
  tokenizer:
    type: hf_llama3
train:
  distributed:
    world_size: 8
    backend: nccl
  checkpointing:
    enabled: true
    save_every_steps: 1000
    keep_last: 1
  per_gpu_batch_size: 8
  max_steps: -1
  learning_rate: 0.0002
  weight_decay: 0.1
  shuffle: true
  lr_scheduler: cosine
  warmup_steps: 1000
  max_tokens: 10000000000
  adam_betas:
  - 0.9
  - 0.95
  adam_eps: 1.0e-08
  grad_clip: 1.0
eval:
  lm_eval:
    tasks:
    - piqa
    - winogrande
    - arc_easy
    - hellaswag
objective:
  type: cross_entropy
