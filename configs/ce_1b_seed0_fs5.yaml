data:
  dataset:
    hf_id: HuggingFaceFW/fineweb-edu
    name: sample-100BT
    split: train
  dir: ./data/mds/
  packing:
    sequence_length: 2048
  text_processing:
    batch_size: 2048
    num_proc: 64
  tokenizer:
    hf_id: meta-llama/Meta-Llama-3-8B
eval:
  every_steps: 2000
  lm_eval:
    limit: 1000
    limit_final: 10000
    tasks:
    - piqa
    - winogrande
    - arc_easy
    - hellaswag
    - charbench
  num_fewshot: 5
model:
  architecture:
    hidden_size: 2048
    intermediate_size: 5504
    max_position_embeddings: 2048
    num_attention_heads: 16
    num_hidden_layers: 16
    num_key_value_heads: 8
    rms_norm_eps: 1.0e-05
    rope_theta: 500000
    tie_word_embeddings: true
  tokenizer:
    type: hf_llama3
objective:
  type: cross_entropy
run:
  name: ce_1b_seed0_fs5
  seed: 0
train:
  adam_betas:
  - 0.9
  - 0.95
  adam_eps: 1.0e-08
  checkpointing:
    enabled: true
    keep_last: 1
    save_every_steps: 2000
  distributed:
    backend: nccl
    world_size: 8
  grad_clip: 1.0
  learning_rate: 0.0002
  lr_scheduler: cosine
  max_steps: -1
  max_tokens: 20000000000
  per_gpu_batch_size: 8
  shuffle: true
  warmup_steps: 1000
  weight_decay: 0.1
