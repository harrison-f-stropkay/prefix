# Llama-3â€“style dense model (~500M params).
architecture:
  hidden_size: 1536
  num_hidden_layers: 11
  num_attention_heads: 12
  num_key_value_heads: 4
  intermediate_size: 4096
  max_position_embeddings: 2048
  rope_theta: 1_000_000
  rms_norm_eps: 1.0e-5
  tie_word_embeddings: true
tokenizer:
  type: hf_llama3

